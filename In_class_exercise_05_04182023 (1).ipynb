{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pctBcN5R97f"
      },
      "source": [
        "# **The fifth in-class-exercise (40 points in total, 4/18/2023)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQZAnrSrR97n"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM \n",
        "\n",
        "(3) KNN \n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "(7) Word2Vec\n",
        "\n",
        "(8) BERT\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison \n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwlwZInqT6Zg",
        "outputId": "b4b2e01b-8254-4bdc-b726-e3fc86466fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "FdfJgMaxR97q",
        "outputId": "211fc554-1811-49d0-9efb-2f82e28c5bf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Text  Label\n",
              "0     a stirring , funny and finally transporting re...      1\n",
              "1     apparently reassembled from the cutting-room f...      0\n",
              "2     they presume their audience wo n't sit still f...      0\n",
              "3     this is a visually stunning rumination on love...      1\n",
              "4     jonathan parker 's bartleby should have been t...      1\n",
              "...                                                 ...    ...\n",
              "6915  painful , horrifying and oppressively tragic ,...      1\n",
              "6916  take care is nicely performed by a quintet of ...      0\n",
              "6917  the script covers huge , heavy topics in a bla...      0\n",
              "6918  a seriously bad film with seriously warped log...      0\n",
              "6919  a deliciously nonsensical comedy about a city ...      1\n",
              "\n",
              "[6920 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3041ec53-749b-4a1d-ac52-06df89c366ab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a stirring , funny and finally transporting re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apparently reassembled from the cutting-room f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they presume their audience wo n't sit still f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is a visually stunning rumination on love...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jonathan parker 's bartleby should have been t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6915</th>\n",
              "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6916</th>\n",
              "      <td>take care is nicely performed by a quintet of ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6917</th>\n",
              "      <td>the script covers huge , heavy topics in a bla...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6918</th>\n",
              "      <td>a seriously bad film with seriously warped log...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6919</th>\n",
              "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6920 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3041ec53-749b-4a1d-ac52-06df89c366ab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3041ec53-749b-4a1d-ac52-06df89c366ab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3041ec53-749b-4a1d-ac52-06df89c366ab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Write your code here\n",
        "import pandas as pd\n",
        "\n",
        "# Read sts-train.txt file\n",
        "with open(\"/content/drive/MyDrive/Excerise five/exercise09_datacollection/stsa-train.txt\", \"r\") as f:\n",
        "    train_lines = f.readlines()\n",
        "\n",
        "train_data = [(line.strip().split(\" \", 1)[1], int(line.strip().split(\" \")[0])) for line in train_lines]\n",
        "train_data = pd.DataFrame(train_data, columns=[\"Text\", \"Label\"])\n",
        "train_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "OcCCYngZVFqU",
        "outputId": "c81abd92-4e8b-4f8c-e361-b0338edf6c7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Text  Label\n",
              "0        no movement , no yuks , not much of anything .      0\n",
              "1     a gob of drivel so sickly sweet , even the eag...      0\n",
              "2     gangs of new york is an unapologetic mess , wh...      0\n",
              "3     we never really feel involved with the story ,...      0\n",
              "4               this is one of polanski 's best films .      1\n",
              "...                                                 ...    ...\n",
              "1816  an often-deadly boring , strange reading of a ...      0\n",
              "1817  the problem with concept films is that if the ...      0\n",
              "1818  safe conduct , however ambitious and well-inte...      0\n",
              "1819  a film made with as little wit , interest , an...      0\n",
              "1820  but here 's the real damn : it is n't funny , ...      0\n",
              "\n",
              "[1821 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47dbefbd-2f60-4fdc-bc7f-27ae9ff00f3e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>no movement , no yuks , not much of anything .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we never really feel involved with the story ,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is one of polanski 's best films .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1816</th>\n",
              "      <td>an often-deadly boring , strange reading of a ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1817</th>\n",
              "      <td>the problem with concept films is that if the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1818</th>\n",
              "      <td>safe conduct , however ambitious and well-inte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1819</th>\n",
              "      <td>a film made with as little wit , interest , an...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1820</th>\n",
              "      <td>but here 's the real damn : it is n't funny , ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1821 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47dbefbd-2f60-4fdc-bc7f-27ae9ff00f3e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-47dbefbd-2f60-4fdc-bc7f-27ae9ff00f3e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-47dbefbd-2f60-4fdc-bc7f-27ae9ff00f3e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Load test data\n",
        "with open(\"/content/drive/MyDrive/Excerise five/exercise09_datacollection/stsa-test.txt\", \"r\") as f:\n",
        "    test_lines = f.readlines()\n",
        "\n",
        "test_data = [(line.strip().split(\" \", 1)[1], int(line.strip().split(\" \")[0])) for line in test_lines]\n",
        "test_data = pd.DataFrame(test_data, columns=[\"Text\", \"Label\"])\n",
        "test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibWTMUz0aBuz",
        "outputId": "972022a2-3f6e-4439-b02d-5e81f8c7b359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.77 (+/- 0.01)\n",
            "Recall: 0.84 (+/- 0.02)\n",
            "Precision: 0.75 (+/- 0.03)\n",
            "F-1 score: 0.79 (+/- 0.01)\n"
          ]
        }
      ],
      "source": [
        "#Multinomial NB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "# Preprocessing data\n",
        "Vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
        "train_vectors = Vectorizer.fit_transform(train_data['Text'])\n",
        "test_vectors = Vectorizer.transform(test_data['Text'])\n",
        "\n",
        "# Splitting train data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_vectors, train_data['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define MultinomialNB classifier\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Cross validation and evaluation\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "scores_accuracy = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n",
        "scores_recall = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='recall')\n",
        "scores_precision = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='precision')\n",
        "scores_f1 = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='f1')\n",
        "\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_accuracy.mean(), scores_accuracy.std()))\n",
        "print(\"Recall: %0.2f (+/- %0.2f)\" % (scores_recall.mean(), scores_recall.std()))\n",
        "print(\"Precision: %0.2f (+/- %0.2f)\" % (scores_precision.mean(), scores_precision.std()))\n",
        "print(\"F-1 score: %0.2f (+/- %0.2f)\" % (scores_f1.mean(), scores_f1.std()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lpUwnl5aMs2",
        "outputId": "0dde1874-7fb4-42fe-c50e-f1afc975b60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Cross Validation Results:\n",
            "Accuracy:  0.7559648389813358\n",
            "Recall:  0.7749492900608519\n",
            "Precision:  0.7627267513372293\n",
            "F1 score:  0.7686086392044941\n"
          ]
        }
      ],
      "source": [
        "#SVM\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import make_scorer, accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "SVM_clf = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'recall': make_scorer(recall_score),\n",
        "           'precision': make_scorer(precision_score),\n",
        "           'f1_score': make_scorer(f1_score)}\n",
        "\n",
        "svm_cv_results = cross_validate(SVM_clf, X_train, y_train, cv=10, scoring=scoring)\n",
        "\n",
        "print(\"SVM Cross Validation Results:\")\n",
        "print(\"Accuracy: \", svm_cv_results['test_accuracy'].mean())\n",
        "print(\"Recall: \", svm_cv_results['test_recall'].mean())\n",
        "print(\"Precision: \", svm_cv_results['test_precision'].mean())\n",
        "print(\"F1 score: \", svm_cv_results['test_f1_score'].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drft6bwua1vP",
        "outputId": "6116beb7-043c-47d2-c5c6-86baa05ce6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Cross Validation Results:\n",
            "Accuracy:  0.49638303706073206\n",
            "Recall:  0.134518553871853\n",
            "Precision:  0.794163317581399\n",
            "F1 score:  0.13635494432206963\n"
          ]
        }
      ],
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "KNN_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "KNN_cv_results = cross_validate(KNN_clf, X_train, y_train, cv=10, scoring=scoring)\n",
        "\n",
        "print(\"KNN Cross Validation Results:\")\n",
        "print(\"Accuracy: \", KNN_cv_results['test_accuracy'].mean())\n",
        "print(\"Recall: \", KNN_cv_results['test_recall'].mean())\n",
        "print(\"Precision: \", KNN_cv_results['test_precision'].mean())\n",
        "print(\"F1 score: \", KNN_cv_results['test_f1_score'].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b28T0XZPbc1g",
        "outputId": "74be8728-6f29-4837-b3e5-397090ccba89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Cross Validation Results:\n",
            "Accuracy:  0.6358347314614736\n",
            "Recall:  0.6934661734876506\n",
            "Precision:  0.640496455661243\n",
            "F1 score:  0.6658023027362413\n"
          ]
        }
      ],
      "source": [
        "#Decision tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "DT_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "DT_cv_results = cross_validate(DT_clf, X_train, y_train, cv=10, scoring=scoring)\n",
        "\n",
        "print(\"Decision Tree Cross Validation Results:\")\n",
        "print(\"Accuracy: \", DT_cv_results['test_accuracy'].mean())\n",
        "print(\"Recall: \", DT_cv_results['test_recall'].mean())\n",
        "print(\"Precision: \", DT_cv_results['test_precision'].mean())\n",
        "print(\"F1 score: \", DT_cv_results['test_f1_score'].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bCf7i5xcHOq",
        "outputId": "a72f3e49-b8fb-40bf-b5e5-91124d3c1354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Cross Validation Results:\n",
            "Accuracy:  0.7149626911953832\n",
            "Recall:  0.7690669371196754\n",
            "Precision:  0.7110539050345186\n",
            "F1 score:  0.7384547614655061\n"
          ]
        }
      ],
      "source": [
        "#Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate the classifier\n",
        "RF_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Cross validation\n",
        "RF_cv_results = cross_validate(RF_clf, X_train.toarray(), y_train, cv=10, scoring=scoring)\n",
        "\n",
        "# Print results\n",
        "print(\"Random Forest Cross Validation Results:\")\n",
        "print(\"Accuracy: \", RF_cv_results['test_accuracy'].mean())\n",
        "print(\"Recall: \", RF_cv_results['test_recall'].mean())\n",
        "print(\"Precision: \", RF_cv_results['test_precision'].mean())\n",
        "print(\"F1 score: \", RF_cv_results['test_f1_score'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRU59UTVcI91"
      },
      "outputs": [],
      "source": [
        "#XGBoost\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# Instantiate the classifier\n",
        "XGB_clf = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "# Cross validation\n",
        "XGB_cv_results = cross_validate(XGB_clf, X_train.toarray(), y_train, cv=10, scoring=scoring)\n",
        "\n",
        "# Print results\n",
        "print(\"XGBoost Cross Validation Results:\")\n",
        "print(\"Accuracy: \", XGB_cv_results['test_accuracy'].mean())\n",
        "print(\"Recall: \", XGB_cv_results['test_recall'].mean())\n",
        "print(\"Precision: \", XGB_cv_results['test_precision'].mean())\n",
        "print(\"F1 score: \", XGB_cv_results['test_f1_score'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPO1p4DbbbKm"
      },
      "outputs": [],
      "source": [
        "# Word2Vec\n",
        "import nltk\n",
        "import gensim\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Preprocess text data\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "preprocessed_text = train_data['Text'].apply(lambda x: [word for word in nltk.word_tokenize(x.lower()) if word.isalpha() and word not in stop_words])\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = gensim.models.Word2Vec(Vectorizer, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Generate word embeddings for each text\n",
        "word_embeddings = []\n",
        "for text in preprocessed_text:\n",
        "    embeddings = []\n",
        "    for word in text:\n",
        "        try:\n",
        "            embeddings.append(model.wv[word])\n",
        "        except KeyError:\n",
        "            pass\n",
        "    word_embeddings.append(embeddings)\n",
        "\n",
        "# Flatten the embeddings list for each text into a single array\n",
        "word_embeddings = [np.concatenate(embeddings) if embeddings else np.zeros(1000) for embeddings in word_embeddings]\n",
        "word_embeddings = np.array(word_embeddings)\n",
        "\n",
        "# Cross validation\n",
        "w2v_cv_results = cross_validate(XGB_clf, word_embeddings, train_data['Label'], cv=10, scoring=scoring)\n",
        "\n",
        "# Print results\n",
        "print(\"Word2Vec Cross Validation Results:\")\n",
        "print(\"Accuracy: \", w2v_cv_results['test_accuracy'].mean())\n",
        "print(\"Precision: \", w2v_cv_results['test_precision'].mean())\n",
        "print(\"Recall: \", w2v_cv_results['test_recall'].mean())\n",
        "print(\"F1 score: \", w2v_cv_results['test_f1_score'].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjW6o16OcN0B"
      },
      "outputs": [],
      "source": [
        "#BERT\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text and get BERT embeddings\n",
        "encoded_text = train_data['Text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, padding='max_length', truncation=True, max_length=512))\n",
        "X_bert = torch.tensor([bert_model(torch.tensor([sample])).last_hidden_state.squeeze(0).mean(dim=0).detach().numpy() for sample in encoded_text])\n",
        "\n",
        "# Set the target variable\n",
        "y = train_data['Label']\n",
        "\n",
        "# Define the XGBoost classifier\n",
        "XGB_clf = xgb.XGBClassifier(random_state=42)\n",
        "\n",
        "# Define the cross-validation method\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and get the results\n",
        "XGB_cv_results = cross_validate(XGB_clf, X_bert, y, cv=cv, scoring=scoring)\n",
        "\n",
        "# Print the results\n",
        "print(\"BERT Cross Validation Results:\")\n",
        "print(\"Accuracy: \", XGB_cv_results['test_accuracy'].mean())\n",
        "print(\"Recall: \", XGB_cv_results['test_recall'].mean())\n",
        "print(\"Precision: \", XGB_cv_results['test_precision'].mean())\n",
        "print(\"F1 score: \", XGB_cv_results['test_f1_score'].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83JVjJrzcPlL",
        "outputId": "dbc60664-76d8-4153-ce4d-dfb885ed996f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultinomialNB\n",
            "Accuracy: 0.79\n",
            "Recall: 0.88\n",
            "Precision: 0.75\n",
            "F-1 score: 0.81\n"
          ]
        }
      ],
      "source": [
        "#final model trained on the test data\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(test_vectors)\n",
        "\n",
        "print(\"MultinomialNB\")\n",
        "print(\"Accuracy: %0.2f\" % accuracy_score(test_data['Label'], y_pred))\n",
        "print(\"Recall: %0.2f\" % recall_score(test_data['Label'], y_pred))\n",
        "print(\"Precision: %0.2f\" % precision_score(test_data['Label'], y_pred))\n",
        "print(\"F-1 score: %0.2f\" % f1_score(test_data['Label'], y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2SAuEieR97t"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K-means\n",
        "\n",
        "DBSCAN\n",
        "\n",
        "Hierarchical clustering\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLDkUeQ4mWZU",
        "outputId": "8be43946-2dd2-45c7-9e78-7be0f7f21e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAQ6yF4ZG8P0",
        "outputId": "76c7f779-a29f-499d-bf27-69cd4f860915"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "from gensim.models import Word2Vec\n",
        "import transformers\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Excerise five/Amazon_Unlocked_Mobile.csv\")\n",
        "\n",
        "# Load the English stopwords list\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Preprocess the text data\n",
        "data['Reviews'] = data['Reviews'].fillna('').str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
        "data['Reviews'] = data['Reviews'].str.lower().apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cXQ97AIYbwc",
        "outputId": "4f0f4cbf-ade8-4f62-9d39-843e46b4efd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K-means silhouette score: 0.4272353510464822\n"
          ]
        }
      ],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "# Sample a subset of the data\n",
        "data_sample = data.sample(n=1000, random_state=42)\n",
        "# Convert the text data to a sparse matrix\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(data_sample['Reviews'])\n",
        "X_sparse = csr_matrix(X)\n",
        "\n",
        "# Apply K-means clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42).fit(X_sparse)\n",
        "kmeans_silhouette_score = silhouette_score(X_sparse, kmeans.labels_)\n",
        "\n",
        "print(f\"K-means silhouette score: {kmeans_silhouette_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTl97qvIHCtd",
        "outputId": "bb74af37-20aa-4d9c-8355-4a82509bc684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DBSCAN silhouette score: -0.28438885579555406\n"
          ]
        }
      ],
      "source": [
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X_sparse)\n",
        "dbscan_silhouette_score = silhouette_score(X_sparse, dbscan.labels_)\n",
        "\n",
        "print(f\"DBSCAN silhouette score: {dbscan_silhouette_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9722yQ0dHBFx",
        "outputId": "24c99962-a798-414d-ea67-cc1828edab63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hierarchical clustering silhouette score: 0.47692301422633604\n"
          ]
        }
      ],
      "source": [
        "# Apply hierarchical clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=5).fit(X_sparse.toarray())\n",
        "agg_silhouette_score = silhouette_score(X_sparse, agg_clustering.labels_)\n",
        "\n",
        "print(f\"Hierarchical clustering silhouette score: {agg_silhouette_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIAeJx6Fbb_y",
        "outputId": "c7b7ba4f-56ef-417a-ed91-2e04d4d6480d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2vec clustering silhouette score: 0.695300281047821\n"
          ]
        }
      ],
      "source": [
        "# Apply Word2Vec clustering\n",
        "sentences = [review.split() for review in data['Reviews']]\n",
        "word2vec_model = Word2Vec(sentences, min_count=5, vector_size=100, workers=4)\n",
        "X_w2v = np.array([word2vec_model.wv.get_vector(term) for term in word2vec_model.wv.index_to_key])\n",
        "kmeans_w2v = KMeans(n_clusters=5, random_state=42).fit(X_w2v)\n",
        "kmeans_w2v_silhouette_score = silhouette_score(X_w2v, kmeans_w2v.labels_)\n",
        "print(f\"Word2vec clustering silhouette score: {kmeans_w2v_silhouette_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhU2QQagfXxt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the text and get BERT embeddings\n",
        "encoded_text = data['Reviews'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, padding='max_length', truncation=True, max_length=512))\n",
        "X_bert = torch.tensor([bert_model(torch.tensor([sample])).last_hidden_state.squeeze(0).mean(dim=0).detach().numpy() for sample in encoded_text])\n",
        "\n",
        "# Apply K-means clustering on the BERT embeddings\n",
        "kmeans_bert = KMeans(n_clusters=5, random_state=42).fit(X_bert)\n",
        "kmeans_bert_silhouette_score = silhouette_score(X_bert, kmeans_bert.labels_)\n",
        "\n",
        "print(f\"BERT clustering silhouette score: {kmeans_bert_silhouette_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWIz2TNTR97v"
      },
      "source": [
        "In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To cluster our data, we employed five distinct clustering methods, and the silhouette score metric was utilized to assess each method's effectiveness. Higher scores denote more well formed clusters, with a silhouette score range of -1 to 1. These approaches created clusters with moderate to good separation, as seen by the silhouette scores of 0.476 for hierarchical clustering, 0.427 for K-means, and 0.695 for Word2Vec. The silhouette score for DBSCAN, on the other hand, was -0.284, which means that the clusters it created were not well defined. Finally, using a silhouette score of 70, we assessed the effectiveness of BERT-based grouping. \n",
        "\n",
        "Overall, our findings imply that BERT outperformed other methods by generating clusters that were highly separable, followed by hierarchical clustering and K-means. To understand why DBSCAN did so poorly in comparison to the other approaches, more research is required."
      ],
      "metadata": {
        "id": "DwA4UHT0Tmgk"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}